# -*- coding: utf-8 -*-
"""NLP_Practical2

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1WRJ1M2F3o7AR4BVpQ32Nhr6G1xpS-aEf
"""

import nltk
from nltk.tokenize import word_tokenize
from nltk.corpus import stopwords
from nltk.stem import PorterStemmer,WordNetLemmatizer

nltk.download('punkt')
nltk.download('stopwords')
nltk.download('wordnet')

text="Natural Language Processing enables machines to understand human Language."

import nltk
nltk.download('punkt_tab')
tokens=word_tokenize(text)
tokens

stop_words=set(stopwords.words('english'))
filtered_tokens=[word for word in tokens if word.lower() not in stop_words]

ps=PorterStemmer()
stemmed_tokens=[ps.stem(word)for word in filtered_tokens]

lemmatizer=WordNetLemmatizer()
lemmatized_tokens=[lemmatizer.lemmatize(word)for word in filtered_tokens]

print("Tokens:",tokens)
print("Filtered Tokkens:",filtered_tokens)
print("Stemmed Tokens:",stemmed_tokens)
print("Lemmatized Tokens:",lemmatized_tokens)